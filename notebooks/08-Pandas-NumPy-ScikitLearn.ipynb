{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas and Scikit-Learn\n",
    "\n",
    "[Pandas](http://pandas.pydata.org) and [scikit-learn](http://scikit-learn.org/stable/) are two of the most popular scientific python libraries.\n",
    "Pandas is commonly used to preprocess, reshape, and transform the data prior to handing it to scikit-learn to fit a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-Minute Intro to Scikit-Learn\n",
    "\n",
    "It's the goto library for machine learning in Python.\n",
    "They use a consistent API for specifiying and fitting models.\n",
    "For *supervised* learning tasks, you have a *feature matrix* `X`, that's an `[N x P]` NumPy array, and a *target array* `y`, that's typically a 1-dimensional array with length `N`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "y = boston['target']\n",
    "X = boston['data']\n",
    "print(boston['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn cleanly separates the *model specification* from the *model fitting*.\n",
    "\n",
    "You specify your model by instantiating an *estimator*, for example `sklearn.linear_model.LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set *hyperparameters* (parameters that are \"outside\", or not learned by the model) when you specify the model. `normalize=True` is a hyperparameter that tells scikit-learn to normalize the data before fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you fit the model by passing the data (feature matrix `X` and target array `y`) to the `.fit` method.\n",
    "At this point, the estimator *learns the parameters* that best fit the data.\n",
    "For a linear regression, that's the `.coef_` attribute, which stores the parameters of the linear model (one per feature, plus an intercept by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "1. Different data models:\n",
    "    - NumPy is homogenous, n-dimensional arrays\n",
    "    - Pandas is heterogenous, 2-dimensional tables\n",
    "3. Pandas has additional dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and scikit-learn have largely overlapping, but still different data models.\n",
    "Scikit-learn uses NumPy arrays for most everything (the exceptiong being scipy sparse matricies for certain tasks, which we'll ignore).\n",
    "Pandas builds on top of NumPy, but has made several extensions to its type system, creating a slight rift between the two. Most notably, pandas supports heterogenous data and has added several extension data-types on top of NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Homogeneity vs. Heterogeneity\n",
    "\n",
    "NumPy `ndarray`s (and so scikit-learn feature matrices) are *homogeneous*, they must have a single dtype, regardless of the number of dimensions.\n",
    "Pandas `DataFrame`s are potentially *heterogenous*, and can store columns of multiple dtypes within a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [10, 1.0],  # mix of integer and floats\n",
    "    [20, 2.0],\n",
    "    [30, 3.0],\n",
    "    ])\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    [10, 1.0],\n",
    "    [20, 2.0],\n",
    "    [30, 3.0]\n",
    "])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extension Types\n",
    "\n",
    "Pandas has implemented some *extension dtypes*: `Categoricals` and datetimes with timezones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These extension types cannot be expressed natively as NumPy arrays, *even if they are a single homogenouse dimension*, and must go through some kind of (potentially lossy) conversion process when converting to NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(pd.Categorical(['a', 'b', 'c', 'a'],\n",
    "                             categories=['d', 'a', 'b', 'c'],\n",
    "                             ordered=True))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting this to a NumPy array loses the categories and ordered information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Real-world\" data is often complex and heterogeneous, making pandas the tool of choice.\n",
    "However, tools like scikit-learn, which do not depend on pandas, can't use its\n",
    "richer data model.\n",
    "\n",
    "In my experience, most of the time the different data models aren't an issue.\n",
    "Recent versions of scikit-learn are much better about taking and returning DataFrames where possible (e.g. `train_test_split`).\n",
    "That said, there are a few rough edges that you can run into.\n",
    "In these cases, we need a way of bridging the gap between pandas' DataFrames and the NumPy arrays appropriate for scikit-learn.\n",
    "Fortunately the tools are all there to make this conversion smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For our example we'll work with a simple dataset on tips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tips.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" data-title=\"Target, Feature arrays\">\n",
    "  <h1><i class=\"fa fa-tasks\" aria-hidden=\"true\"></i> Exercise: Target, Feature arrays</h1>\n",
    "</div>\n",
    "<p>Split the DataFrame `df` into a `Series` called `y` containing the `tip` amount, and a DataFrame `X` containing everything else.\n",
    "\n",
    "Our target variable is the tip amount. The remainder of the columns make up our features.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/sklearn_pandas_split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the feature matrix is a mixture of numeric and categorical variables.\n",
    "In statistics, a categorical variable is a variable that comes from a limited, fixed set of values.\n",
    "At the moment though, the actual data-type of those columns is just `object`, containing python strings. We'll convert those to pandas `Categorical`s later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stats\n",
    "\n",
    "Our focus is about how to use pandas and scikit-learn together, not how to build the best tip-predicting model.\n",
    "To keep things simple, we'll fit a linear regression to predict `tip`, rather than some more complicated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you fit a linear regression, you (or scikit-learn, rather) end up having to solve an equation to find the line that minimizes the mean squared error between the predictions and observations. The equation that gives the best-fit line is\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\beta}} = \\left(\\boldsymbol{X}^T\\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\hat{\\boldsymbol{\\beta}}$ is our estimate for the vector of coefficients describing the best-fit line (`LinearRegression.coef_`)\n",
    "- $\\boldsymbol{X}$ is the feature matrix\n",
    "- $\\boldsymbol{y}$ is the target array (tip amount)\n",
    "\n",
    "There's no need to worry about that equation; it likely won't make sense unless you've seen it before.\n",
    "The only point I want to emphasize is that finding the best-fit line requires doing some matrix multiplications.\n",
    "If we just tried to fit a regression on our raw data, we'd get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%xmode Plain\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The message, \"could not convert string to float\" says it all.\n",
    "We (or our library) need to somehow convert our *categorical* data (`sex`, `smoker`, `day`, and `time`) into numeric data.\n",
    "The next two sections offer some possible ways of doing that conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Encoding\n",
    "\n",
    "![dummy](figures/dummy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy encoding is one approach to converting categorical to numeric data.\n",
    "It expands each categorical column to *multiple* columns, one per distinct value.\n",
    "The values in these new dummy-encoded columns are either 1, indicating the presence of that value in that observation, or 0.\n",
    "Versions of this are implemented in both scikit-learn and pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend the pandas version, `get_dummies`. It offers a few conveniences:\n",
    "\n",
    "- Operates on multiple columns at once\n",
    "- Passes through numeric columns unchanged\n",
    "- Preserves row and column labels\n",
    "- Provides a `drop_first` keyword for dropping a level per column. You might want this to avoid [perfect multicolinearity](https://en.wikipedia.org/wiki/Multicollinearity) if you have an intercept\n",
    "- Uses Categorical information (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dummy = pd.get_dummies(X)\n",
    "X_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_dummy, y)\n",
    "\n",
    "pd.Series(lm.coef_, index=X_dummy.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinements\n",
    "\n",
    "Our last approach worked, but there's still room for improvement.\n",
    "\n",
    "1. We can't easily go from dummies back to categoricals\n",
    "2. Doesn't integrate with scikit-learn `Pipeline` objects.\n",
    "3. If working with a larger dataset and `partial_fit`, codes could be missing from subsets of the data.\n",
    "4. Memory inefficient if there are many records relative to distinct categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These items become more important when you go to \"productionize\" your model.\n",
    "But keep in mind that we've solved the basic problem of moving from pandas DataFrames to NumPy arrays for scikit-learn; now we're just making the bridge sturdier.\n",
    "\n",
    "To accomplish this we'll store additonal information in the *type* of the column and write a [Transformer](http://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) to handle the conversion to and from dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: scikit-learn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rarely when doing data analysis do we take plug a raw dataset directly into a model.\n",
    "There's typically some preprocessing and feature engineering before the fitting stage.\n",
    "`scikit-learn` provides the `Pipeline` interface for chaining together a sequence of fit and transform steps. For example, suppose we wanted our pipeline to be\n",
    "\n",
    "- standardize each column (subtract the mean, normalize the variance to 1)\n",
    "- compute all the [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics))\n",
    "- fitting a Lasso regression\n",
    "\n",
    "Without using a scikit-learn `Pipeline`, you need to assign the output of each step to a temporary variable, and manually shuttling data through to the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = StandardScaler().fit_transform(X_dummy, y)\n",
    "X_poly = (PolynomialFeatures(interaction_only=True)\n",
    "            .fit_transform(X_scaled, y))\n",
    "model = Lasso(alpha=.5)\n",
    "model.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pipelines, this becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(),\n",
    "                     PolynomialFeatures(interaction_only=True),\n",
    "                     Lasso(alpha=.5))\n",
    "pipe.fit(X_dummy, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always recommend the pipeline version.\n",
    "For one thing, I prefer the aesthetics.\n",
    "Especially with longer chains of computations, pipelines remove the need for many temporary variables.\n",
    "They are also easier to save to disk with [joblib](https://pythonhosted.org/joblib/persistence.html).\n",
    "But the most important reason is the interaction of `Pipeline` and [`GridSearchCV`](http://scikit-learn.org/stable/modules/grid_search.html).\n",
    "\n",
    "When fitting a model you'll typically have a space of *hyperparameters* to search over.\n",
    "These are the parameters passed to each estimators `__init__` method, so before the `.fit` step.\n",
    "In the pipeline above, some examples of hyperparameters are the `interaction_only` parameter of `PolynomialFeatures` and the `alpha` parameter of `Lasso`.\n",
    "\n",
    "A common mistake in machine learning is to let information from your test dataset leak into your training dataset by preprocessing *before* splitting.\n",
    "This means the score you get on the test may not be an accurate representation of the score you'll get on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` provides many tools for you to write custom transformers that work well in its `Pipeline`.\n",
    "When writing a custom transformer, you should:\n",
    "\n",
    "- inherit from `sklearn.base.TransformerMixin`\n",
    "- implement a `.fit` method that takes a feature matrix `X` and a target array `y`, returning `self`.\n",
    "- implement a `.transform` method that also takes an `X` and a `y`, returning the transformed feature matrix\n",
    "\n",
    "Below, we'll write a couple custom transformers to make our last regression more robust. But before that, we need to examine one of pandas' extension dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas `Categorical` dtype\n",
    "\n",
    "We've already talked about Categoricals, but as a refresher:\n",
    "\n",
    "- There are a fixed set of possible values the variable can take\n",
    "- The cateogories can be ordered or unordered\n",
    "- The array of data is dictionary encoded, so the set of possible values is stored once, and the array of actual values is stored efficiently as an array of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Categorical`s can be constructed either with the `pd.Categorical` constructor, or using the `.astype` method on a `Series`. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day = df['day'].astype('category').head()\n",
    "day.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.astype('category')` we're just using the defaults of\n",
    "\n",
    "- The set of categories is just the set present in the column\n",
    "- There is no ordering\n",
    "\n",
    "The categorical-specific information of a `Series` is stored under the `.cat` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day.cat.ordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class is a transformer that transforms to categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class CategoricalTransformer(TransformerMixin):\n",
    "    \"Converts a set of columns in a DataFrame to categoricals\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        'Records the categorical information'\n",
    "        self.cat_map_ = {col: X[col].astype('category').cat\n",
    "                         for col in self.columns}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for col in self.columns:\n",
    "            X[col] = pd.Categorical(X[col],\n",
    "                                    categories=self.cat_map_[col].categories,\n",
    "                                    ordered=self.cat_map_[col].ordered)\n",
    "        return X\n",
    "    \n",
    "    def inverse_transform(self, trn, y=None):\n",
    "        trn = trn.copy()\n",
    "        trn[self.columns] = trn[self.columns].apply(lambda x: x.astype(object))\n",
    "        return trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important rule when writing custom objects to be used in a `Pipeline` is that the `transfrom` and `inverse_transform` steps shouldn't modify `self`. That should only occur in `fit`. Because we inherited from `TransformerMixin`, we get the `fit_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CategoricalTransformer(columns=['sex', 'smoker', 'day', 'time'])\n",
    "X_cat = ct.fit_transform(X)\n",
    "X_cat.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DummyEncoder\n",
    "\n",
    "We now have the pieces in place to solve all our issues.\n",
    "We'll write a class `DummyEncoder` for use in a scikit-learn `Pipeline`.\n",
    "The entirety is given in the next cell, but we'll break it apart piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEncoder(TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.columns_ = X.columns\n",
    "        self.cat_cols_ = X.select_dtypes(include=['category']).columns\n",
    "        self.non_cat_cols_ = X.columns.drop(self.cat_cols_)\n",
    "        self.cat_map_ = {col: X[col].cat for col in self.cat_cols_}\n",
    "        \n",
    "        self.cat_blocks_ = {}  # {cat col: slice}\n",
    "        left = len(self.non_cat_cols_)\n",
    "        for col in self.cat_cols_:\n",
    "            right = left + len(self.cat_map_[col].categories)\n",
    "            self.cat_blocks_[col] = slice(left, right)\n",
    "            left = right\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.asarray(pd.get_dummies(X))\n",
    "    \n",
    "    def inverse_transform(self, trn, y=None):\n",
    "        numeric = pd.DataFrame(trn[:, :len(self.non_cat_cols_)],\n",
    "                               columns=self.non_cat_cols_)\n",
    "        series = []\n",
    "        for col, slice_ in self.cat_blocks_.items():\n",
    "            codes = trn[:, slice_].argmax(1)\n",
    "            cat = self.cat_map_[col]\n",
    "            cat = pd.Categorical.from_codes(codes,\n",
    "                                            cat.categories,\n",
    "                                            cat.ordered)\n",
    "            series.append(pd.Series(cat, name=col))\n",
    "        return pd.concat([numeric] + series, axis='columns')[self.columns_]\n",
    "\n",
    "self = DummyEncoder()\n",
    "trn = self.fit_transform(X_cat)\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.transform`\n",
    "\n",
    "The transform method is the simplest, it's using `pd.get_dummies` like we did before.\n",
    "That is wrapped in a `np.asarray` to convert the DataFrame to a NumPy array, simulating what would happen if we pass the dummy-encoded class to a scikit-learn transformer that deals with NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.fit`\n",
    "\n",
    "In `.fit`, we need to store all the information needed to take the `trn` NumPy array and go back to a `DataFrame` in the `.inverse_transform` step. This includes\n",
    "\n",
    "- Column names (`self.columns_`)\n",
    "- Cateogrical information (`self.cat_map_`)\n",
    "- Mapping original columns to transformed positions (`self.non_cat_cols_` and `self.cat_blocks_`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numeric\n",
    "\n",
    "The first thing to realize is that pandas `get_dummies` returns the un-touched (numeric) columns first. We had two of those, `total_bill` and `size`, and collect those first in `inverse_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = pd.DataFrame(trn[:, :len(self.non_cat_cols_)],\n",
    "                       columns=self.non_cat_cols_)\n",
    "numeric.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we had two columns in this dataset, in general we'll have `len(self.non_cat_cols_)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categoricals\n",
    "\n",
    "The rest of `inverse_transform` deals with `categoricals`.\n",
    "We have two separate tasks here.\n",
    "\n",
    "1. Know which of the expanded columns in `trn` belong to which original categorical columns\n",
    "2. Know the categorical attributes (`ordered`, `categories`) for that categorical\n",
    "\n",
    "For the first task, we use the information stored in `self.cat_blocks_`. This is a dictonary mapping categorical column names to `slice` objects, that can be used on `trn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cat_blocks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn[:, self.cat_blocks_['day']][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these \"blocks\" contain the dummy-encoded categorical. To invert the dummy-encoded, we use the fact that each row only has a single `1`, meaning that the maximum value of the row is the category for that row. We'll find the `argmax` for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = trn[:, self.cat_blocks_['day']].argmax(1)\n",
    "codes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the alternative `Categorical.from_codes` constructor, which is appropriate when you have an array of `codes`, and already know the `categories` and `ordered`. We've stored that categorical information in the `self.cat_map_` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.Categorical.from_codes(codes, self.cat_map_['day'].categories,\n",
    "                                self.cat_map_['day'].ordered)\n",
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of `inverse_transform` collects all the series of `Categorical` and concats them together with the `numeric` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sex', 'smoker', 'day', 'time']\n",
    "pipe = make_pipeline(CategoricalTransformer(columns), DummyEncoder(), LinearRegression())\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = pipe.predict(X)\n",
    "sns.jointplot(y, y-yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(CategoricalTransformer(columns), DummyEncoder(), PCA())\n",
    "trn = pipe.fit_transform(X)\n",
    "sns.jointplot(trn[:, 0], trn[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.inverse_transform(trn).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We explored some of the differences between the scikit-learn (NumPy) and pandas data models.\n",
    "We needed to convert a heterogenous pandas `DataFrame` to a homogonous `ndarray` for use in scikit-learn.\n",
    "Specifically we used `pd.get_dummies` to dummy encode the categorical data.\n",
    "After dummy encoding, we sucessfully fit the model. \n",
    "\n",
    "For a more robust method, we implmented two scikit-learn `Pipeline` compatible transformers.\n",
    "The first converted columns of strings into proper pandas `Categorical`s.\n",
    "The second used `pd.get_dummies` to transform `Categoricals`, storing all the information needed to reverse the transformation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
